<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 900 700">
  <defs>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feOffset dx="2" dy="2" in="SourceAlpha" result="offsetblur"/>
      <feGaussianBlur in="offsetblur" stdDeviation="3" result="blur"/>
      <feFlood flood-color="#000" flood-opacity="0.3" result="color"/>
      <feComposite in="color" in2="blur" operator="in" result="shadow"/>
      <feMerge>
        <feMergeNode in="shadow"/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
  </defs>
  
  <!-- Background -->
  <rect width="900" height="700" fill="#0f172a"/>
  
  <!-- Title -->
  <text x="450" y="35" font-family="Arial, sans-serif" font-size="22" fill="#fff" text-anchor="middle" font-weight="bold" dominant-baseline="middle">
    Attention Mechanism Formulas (from Paper)
  </text>
  
  <!-- Equation 1: Basic Attention -->
  <g transform="translate(30, 70)">
    <rect x="0" y="0" width="400" height="100" rx="12" fill="#1e293b" stroke="#ef4444" stroke-width="2" filter="url(#shadow)"/>
    <text x="200" y="25" font-family="Arial" font-size="13" fill="#ef4444" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equation (1): Basic Attention</text>
    
    <text x="200" y="55" font-family="Georgia, serif" font-size="16" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      Attention(Q, K, V) = Score(Q, K) V
    </text>
    
    <text x="200" y="85" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">
      Score function computes compatibility between tokens
    </text>
  </g>
  
  <!-- Equation 2: Softmax -->
  <g transform="translate(470, 70)">
    <rect x="0" y="0" width="400" height="100" rx="12" fill="#1e293b" stroke="#4facfe" stroke-width="2" filter="url(#shadow)"/>
    <text x="200" y="25" font-family="Arial" font-size="13" fill="#4facfe" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equation (2): Softmax Function</text>
    
    <text x="200" y="55" font-family="Georgia, serif" font-size="16" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      Softmax(x)_i = exp(x_i) / Sum_j exp(x_j)
    </text>
    
    <text x="200" y="85" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">
      Converts scores to probability distribution
    </text>
  </g>
  
  <!-- Equation 3: Scaled Dot-Product Attention -->
  <g transform="translate(30, 190)">
    <rect x="0" y="0" width="840" height="120" rx="12" fill="#1e293b" stroke="#43e97b" stroke-width="2" filter="url(#shadow)"/>
    <text x="420" y="28" font-family="Arial" font-size="14" fill="#43e97b" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equation (3): Scaled Dot-Product Attention</text>
    
    <text x="420" y="65" font-family="Georgia, serif" font-size="20" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      Attention(Q, K, V) = Softmax(QK^T / sqrt(d)) V
    </text>
    
    <text x="150" y="100" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">Q, K, V in R^(n x d)</text>
    <text x="420" y="100" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">QK^T in R^(n x n) - O(n^2) complexity!</text>
    <text x="700" y="100" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">sqrt(d) prevents gradient vanishing</text>
  </g>
  
  <!-- Equations 4-5: Multi-Head Attention -->
  <g transform="translate(30, 330)">
    <rect x="0" y="0" width="840" height="150" rx="12" fill="#1e293b" stroke="#a78bfa" stroke-width="2" filter="url(#shadow)"/>
    <text x="420" y="28" font-family="Arial" font-size="14" fill="#a78bfa" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equations (4-5): Multi-Head Attention</text>
    
    <text x="420" y="65" font-family="Georgia, serif" font-size="16" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      MultiHead(Q, K, V) = [head_1; ...; head_h] W^O
    </text>
    
    <text x="420" y="100" font-family="Georgia, serif" font-size="14" fill="#e2e8f0" text-anchor="middle" dominant-baseline="middle">
      head_i = Softmax(QW_i^Q (KW_i^K)^T / sqrt(d_k)) VW_i^V
    </text>
    
    <text x="200" y="135" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">W_i^Q, W_i^K in R^(d x d_k)</text>
    <text x="450" y="135" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">W_i^V in R^(d x d_v)</text>
    <text x="700" y="135" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">W^O in R^(h*d_v x d)</text>
  </g>
  
  <!-- Equations 6-7: Encoder Layer -->
  <g transform="translate(30, 500)">
    <rect x="0" y="0" width="400" height="130" rx="12" fill="#1e293b" stroke="#f093fb" stroke-width="2" filter="url(#shadow)"/>
    <text x="200" y="25" font-family="Arial" font-size="13" fill="#f093fb" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equations (6-7): Encoder Layer</text>
    
    <text x="200" y="60" font-family="Georgia, serif" font-size="14" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      X_A = LayerNorm(Attention(Q,K,V) + X)
    </text>
    
    <text x="200" y="90" font-family="Georgia, serif" font-size="14" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      X_B = LayerNorm(FFN(X_A) + X_A)
    </text>
    
    <text x="200" y="118" font-family="monospace" font-size="9" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">Residual connections + Layer Normalization</text>
  </g>
  
  <!-- Equation 8: FFN -->
  <g transform="translate(470, 500)">
    <rect x="0" y="0" width="400" height="130" rx="12" fill="#1e293b" stroke="#fbbf24" stroke-width="2" filter="url(#shadow)"/>
    <text x="200" y="25" font-family="Arial" font-size="13" fill="#fbbf24" text-anchor="middle" font-weight="bold" dominant-baseline="middle">Equation (8): Feed-Forward Network</text>
    
    <text x="200" y="60" font-family="Georgia, serif" font-size="14" fill="#fff" text-anchor="middle" dominant-baseline="middle">
      FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
    </text>
    
    <text x="200" y="90" font-family="monospace" font-size="10" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">
      W_1 in R^(d x d_f), W_2 in R^(d_f x d)
    </text>
    
    <text x="200" y="118" font-family="monospace" font-size="9" fill="#94a3b8" text-anchor="middle" dominant-baseline="middle">ReLU activation, applied position-wise</text>
  </g>
  
  <!-- Equation 9: LayerNorm -->
  <g transform="translate(30, 650)">
    <rect x="0" y="0" width="840" height="40" rx="8" fill="#1e293b" stroke="#64748b" stroke-width="1"/>
    <text x="100" y="25" font-family="Arial" font-size="11" fill="#64748b" font-weight="bold" dominant-baseline="middle">Eq (9): LayerNorm</text>
    <text x="480" y="25" font-family="Georgia, serif" font-size="13" fill="#e2e8f0" text-anchor="middle" dominant-baseline="middle">
      LayerNorm(x) = g * (x - mu) / sqrt(sigma^2 + epsilon) + b
    </text>
  </g>
</svg>

