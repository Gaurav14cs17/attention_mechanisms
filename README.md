<div align="center">

<!-- HERO SECTION -->
<img src="https://img.shields.io/badge/ğŸ“š_Surveys-3-blue?style=for-the-badge" alt="Surveys"/>
<img src="https://img.shields.io/badge/ğŸ¨_Diagrams-55+-purple?style=for-the-badge" alt="Diagrams"/>
<img src="https://img.shields.io/badge/â“_Q&A-130+-green?style=for-the-badge" alt="Q&A"/>

# âš¡ Attention Mechanisms
### The Complete Visual Encyclopedia

---

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•
                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
                              â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•
                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘      â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• 
                              â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘       â•šâ–ˆâ–ˆâ•”â•  
                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   
                              â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•   â•šâ•â•   
```

*Three in-depth surveys covering everything you need to know about efficient transformer architectures*

**From O(nÂ²) â†’ O(n) | FlashAttention â†’ Mamba | ViT â†’ DINOv2**

[ğŸ“– Efficient Attention](#-efficient-attention-survey) Â· [âš¡ Faster Transformers](#-faster-and-lighter-transformers) Â· [ğŸ‘ï¸ Vision Transformers](#ï¸-vision-transformers)

</div>

---

## ğŸ¯ What's Inside?

<table>
<tr>
<td width="33%" align="center">

### ğŸ“– Efficient Attention

**19 Diagrams**

FlashAttention â€¢ GQA/MQA  
Sparse Patterns â€¢ Mamba  

</td>
<td width="33%" align="center">

### âš¡ Faster & Lighter

**29 Diagrams**

Quantization â€¢ Pruning  
Distillation â€¢ MoE  

</td>
<td width="33%" align="center">

### ğŸ‘ï¸ Vision Transformers

**16 Diagrams**

ViT â€¢ Swin â€¢ MAE  
DINO â€¢ 130+ Q&A  

</td>
</tr>
</table>

---

<div align="center">

## ğŸ“š Survey Collection

</div>

---

## ğŸ“– Efficient Attention Survey

<table>
<tr>
<td width="70px" align="center">
<h1>ğŸ”¥</h1>
</td>
<td>

### Hardware-efficient, Sparse, Compact, and Linear Attention

**The definitive guide to making attention O(n) without sacrificing quality**

</td>
<td width="120px" align="center">

[![Read](https://img.shields.io/badge/Read-Blog-blue?style=for-the-badge)](./Efficient_Attention_Survey/BLOG_README.md)

</td>
</tr>
</table>

<table>
<tr>
<td width="50%">

#### ğŸ—ï¸ Four Pillars of Efficient Attention

| Class | Core Idea | Key Methods |
|:-----:|:----------|:------------|
| âš¡ | **Hardware-efficient** | FlashAttention, SageAttention |
| ğŸ“¦ | **Compact** | MQA, GQA, MLA (KV compression) |
| ğŸ¯ | **Sparse** | Longformer, BigBird, H2O |
| ğŸ”„ | **Linear** | Mamba, RWKV, RetNet |

</td>
<td width="50%">

#### ğŸ“‹ You'll Learn

```
âœ“ GPU memory hierarchy (HBM vs SRAM)
âœ“ Prefilling vs Decoding optimization
âœ“ KV cache compression techniques
âœ“ Gating mechanisms in linear attention
âœ“ Test-Time Training (TTT)
```

</td>
</tr>
</table>

<details>
<summary><b>ğŸ“Š View All 19 Visualizations</b></summary>

| | Diagram | Description |
|:-:|:--------|:------------|
| 1 | `overview_attention_types.svg` | Four classes of efficient attention |
| 2 | `standard_attention_explained.svg` | Step-by-step attention mechanism |
| 3 | `gpu_memory_hierarchy.svg` | HBM vs SRAM explained |
| 4 | `flash_attention.svg` | FlashAttention tiling strategy |
| 5 | `compact_attention.svg` | MQA, GQA, MLA comparison |
| 6 | `sparse_attention.svg` | Sparse attention patterns |
| 7 | `linear_attention_forms.svg` | Parallel, Recurrent, Chunkwise |
| 8 | `linear_attention_methods.svg` | Mamba, RWKV, RetNet |
| 9 | `gating_mechanisms.svg` | Forget and select gates |
| 10 | `test_time_training.svg` | TTT approach |
| 11-19 | `formula_*.svg` | Mathematical formulations |

</details>

---

## âš¡ Faster and Lighter Transformers

<table>
<tr>
<td width="70px" align="center">
<h1>ğŸš€</h1>
</td>
<td>

### A Practical Survey on Making Transformers Deployable

**Distillation â†’ Pruning â†’ Quantization = 40-85Ã— compression**

</td>
<td width="120px" align="center">

[![Read](https://img.shields.io/badge/Read-Blog-green?style=for-the-badge)](./Faster%20and%20Lighter%20Transformers/README.md)

</td>
</tr>
</table>

<table>
<tr>
<td width="50%">

#### ğŸ› ï¸ Complete Efficiency Toolkit

| Method | What It Does | Savings |
|:------:|:-------------|:-------:|
| ğŸ“š | **Knowledge Distillation** | 5-7Ã— smaller |
| ğŸ”¢ | **Quantization** | 4-32Ã— smaller |
| âœ‚ï¸ | **Pruning** | 2-4Ã— smaller |
| ğŸ”— | **Weight Sharing** | 12-18Ã— smaller |
| ğŸ”€ | **MoE** | 5Ã— faster training |
| ğŸ’¾ | **Checkpointing** | 10Ã— less memory |

</td>
<td width="50%">

#### ğŸ“‹ You'll Learn

```
âœ“ DistilBERT, TinyBERT, MobileBERT
âœ“ INT8, Mixed Precision, QAT
âœ“ Structured vs Unstructured Pruning
âœ“ ALBERT's 18Ã— parameter reduction
âœ“ Switch Transformer & trillion-scale
âœ“ Combining techniques effectively
```

</td>
</tr>
</table>

<details>
<summary><b>ğŸ“Š View All 29 Visualizations</b></summary>

| | Diagram | Description |
|:-:|:--------|:------------|
| 1-5 | Architecture | Overview, Transformer, RNN comparison |
| 6-10 | Formulas | Attention equations explained |
| 11-15 | Distillation | Teacher-student, KD loss |
| 16-20 | Compression | Quantization, Pruning methods |
| 21-25 | Efficiency | Weight sharing, MoE, GPipe |
| 26-29 | Comparison | Model sizes, complexity tables |

</details>

---

## ğŸ‘ï¸ Vision Transformers

<table>
<tr>
<td width="70px" align="center">
<h1>ğŸ‘ï¸</h1>
</td>
<td>

### The Complete Visual Guide to Vision Transformers

**ViT â†’ DeiT â†’ Swin â†’ MAE â†’ DINOv2 + 130 Interview Questions**

</td>
<td width="120px" align="center">

[![Read](https://img.shields.io/badge/Read-Blog-purple?style=for-the-badge)](./Vision_Transformers/README.md)

</td>
</tr>
</table>

<table>
<tr>
<td width="50%">

#### ğŸ—ºï¸ Complete ViT Landscape

| Category | Key Models |
|:--------:|:-----------|
| ğŸ”· | **Core ViT** â€” ViT, DeiT, ViT-H/G |
| ğŸ—ï¸ | **Hierarchical** â€” Swin, CSWin, Focal |
| ğŸ”€ | **Hybrid** â€” CvT, ConViT, CoAtNet, LeViT |
| ğŸ“± | **Mobile** â€” PVT, MobileViT, EdgeViT |
| ğŸ“ | **Self-Supervised** â€” MAE, BEiT, DINO |
| ğŸ¯ | **Tasks** â€” DETR, Mask2Former |

</td>
<td width="50%">

#### ğŸ“‹ You'll Learn

```
âœ“ Patch embedding and [CLS] token
âœ“ Window-based attention (Swin)
âœ“ Mobile deployment strategies
âœ“ Self-supervised pre-training
âœ“ Object detection with DETR
âœ“ 130+ interview Q&A with diagrams
```

</td>
</tr>
</table>

<details>
<summary><b>ğŸ“Š View All 16 Visualizations</b></summary>

**Main Diagrams (7):**
| | Diagram | Description |
|:-:|:--------|:------------|
| 1 | `vit_taxonomy.svg` | Complete ViT taxonomy |
| 2 | `core_vit_architecture.svg` | ViT architecture |
| 3 | `swin_transformer.svg` | Hierarchical Swin |
| 4 | `hybrid_cnn_transformer.svg` | CNN + Transformer |
| 5 | `efficient_mobile_vit.svg` | Mobile variants |
| 6 | `self_supervised_vit.svg` | MAE, DINO |
| 7 | `vision_tasks.svg` | Detection, Segmentation |

**Interview Q&A Diagrams (9):**
| | Diagram | Topic |
|:-:|:--------|:------|
| 1 | `patch_embedding_process.svg` | How patches work |
| 2 | `cls_token_explained.svg` | [CLS] token role |
| 3 | `vit_vs_cnn_comparison.svg` | ViT vs CNN |
| 4 | `position_encoding_types.svg` | Position encodings |
| 5 | `attention_complexity.svg` | O(nÂ²) explained |
| 6 | `window_vs_global_attention.svg` | Local vs global |
| 7 | `multi_head_attention.svg` | Multi-head mechanism |
| 8 | `mae_pretraining.svg` | MAE approach |
| 9 | `deit_distillation.svg` | DeiT training |

</details>

---

<div align="center">

## ğŸ§­ Quick Navigation

</div>

### ğŸ”¥ Hot Topics

| Topic | Survey | Quick Link |
|:------|:------:|:-----------|
| **FlashAttention** | Efficient Attention | [â†’ Hardware-efficient](./Efficient_Attention_Survey/BLOG_README.md#-class-1-hardware-efficient-attention) |
| **KV Cache (GQA/MQA)** | Efficient Attention | [â†’ Compact Attention](./Efficient_Attention_Survey/BLOG_README.md#-class-2-compact-attention) |
| **Mamba/RWKV** | Efficient Attention | [â†’ Linear Attention](./Efficient_Attention_Survey/BLOG_README.md#-class-4-linear-attention) |
| **Quantization (INT8)** | Faster & Lighter | [â†’ Quantization](./Faster%20and%20Lighter%20Transformers/README.md#quantization) |
| **DistilBERT** | Faster & Lighter | [â†’ Distillation](./Faster%20and%20Lighter%20Transformers/README.md#knowledge-distillation) |
| **Swin Transformer** | Vision Transformers | [â†’ Hierarchical ViT](./Vision_Transformers/README.md#-hierarchical-vision-transformers) |
| **MAE / DINO** | Vision Transformers | [â†’ Self-Supervised](./Vision_Transformers/README.md#-self-supervised-vision-transformers) |
| **Interview Prep** | Vision Transformers | [â†’ 130+ Q&A](./Vision_Transformers/interview_qa/README.md) |

---

<div align="center">

## ğŸ“Š Method Comparison

</div>

### âš¡ Efficiency Trade-offs

| Approach | Speed | Memory | Quality | Effort |
|:---------|:-----:|:------:|:-------:|:------:|
| **FlashAttention** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Drop-in |
| **GQA/MQA** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Retrain |
| **Sparse Attention** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | Retrain |
| **Linear Attention** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | Full train |
| **Quantization** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | PTQ/QAT |
| **Pruning** | â­â­â­ | â­â­â­â­ | â­â­â­ | Fine-tune |
| **Distillation** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | Full train |
| **MoE** | â­â­â­â­ | â­â­ | â­â­â­â­â­ | Full train |

### ğŸ¯ When to Use What?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECISION TREE: Which efficiency method should I use?               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â“ Need to handle long sequences (>2K tokens)?                     â”‚
â”‚     â”œâ”€â”€ YES â†’ FlashAttention (drop-in) or Sparse/Linear attention  â”‚
â”‚     â””â”€â”€ NO  â†’ Standard attention is fine                           â”‚
â”‚                                                                     â”‚
â”‚  â“ Need smaller model for deployment?                              â”‚
â”‚     â”œâ”€â”€ YES â†’ Distillation â†’ Pruning â†’ Quantization (combine!)     â”‚
â”‚     â””â”€â”€ NO  â†’ Keep original model                                  â”‚
â”‚                                                                     â”‚
â”‚  â“ Limited memory during training?                                 â”‚
â”‚     â”œâ”€â”€ YES â†’ Gradient checkpointing + Mixed precision             â”‚
â”‚     â””â”€â”€ NO  â†’ Standard training                                    â”‚
â”‚                                                                     â”‚
â”‚  â“ Working with images?                                            â”‚
â”‚     â”œâ”€â”€ YES â†’ Swin (dense) or ViT (classification)                 â”‚
â”‚     â””â”€â”€ NO  â†’ Text transformers                                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<div align="center">

## ğŸ“ Repository Structure

</div>

```
attention_mechanisms/
â”‚
â”œâ”€â”€ ğŸ“– README.md                              â† You are here
â”‚
â”œâ”€â”€ ğŸ“‚ Efficient_Attention_Survey/
â”‚   â”œâ”€â”€ BLOG_README.md                        â† Main blog post
â”‚   â”œâ”€â”€ README.md                             â† Technical summary
â”‚   â”œâ”€â”€ svg_figs/                             â† 19 diagrams
â”‚   â”œâ”€â”€ png_figs/                             â† Original paper figures
â”‚   â””â”€â”€ resource/                             â† Source PDFs
â”‚
â”œâ”€â”€ ğŸ“‚ Faster and Lighter Transformers/
â”‚   â”œâ”€â”€ README.md                             â† Main blog post
â”‚   â”œâ”€â”€ svg_figs/                             â† 29 diagrams
â”‚   â””â”€â”€ *.pdf                                 â† Source paper
â”‚
â””â”€â”€ ğŸ“‚ Vision_Transformers/
    â”œâ”€â”€ README.md                             â† Main blog post
    â”œâ”€â”€ svg_figs/                             â† 7 diagrams
    â””â”€â”€ interview_qa/
        â”œâ”€â”€ README.md                         â† 130+ Q&A
        â””â”€â”€ svg_figs/                         â† 9 explanatory diagrams
```

---

<div align="center">

## ğŸš€ Getting Started

</div>

### ğŸ“š Learning Path

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        NEW TO TRANSFORMERS?         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  1. Understand the O(nÂ²) Problem    â”‚
                    â”‚     â†’ Efficient Attention Survey    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TEXT/LLMs?      â”‚   â”‚    DEPLOYMENT?        â”‚   â”‚     VISION?       â”‚
â”‚                   â”‚   â”‚                       â”‚   â”‚                   â”‚
â”‚ Linear Attention  â”‚   â”‚ Distillation +        â”‚   â”‚ ViT â†’ Swin â†’      â”‚
â”‚ FlashAttention    â”‚   â”‚ Quantization          â”‚   â”‚ Mobile ViT        â”‚
â”‚ GQA/MQA           â”‚   â”‚ Pruning               â”‚   â”‚ MAE/DINO          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“– Recommended Reading Order

| Step | What | Link |
|:----:|:-----|:-----|
| 1ï¸âƒ£ | Understand the quadratic bottleneck | [Efficient Attention â†’ Problem](./Efficient_Attention_Survey/BLOG_README.md#-the-quadratic-bottleneck-problem) |
| 2ï¸âƒ£ | Learn standard attention step-by-step | [Efficient Attention â†’ Standard](./Efficient_Attention_Survey/BLOG_README.md#-standard-attention-step-by-step) |
| 3ï¸âƒ£ | Explore all efficiency methods | [Faster & Lighter â†’ Taxonomy](./Faster%20and%20Lighter%20Transformers/README.md#taxonomy-of-efficiency-methods) |
| 4ï¸âƒ£ | Vision Transformer fundamentals | [Vision â†’ Core ViT](./Vision_Transformers/README.md#-core-vit-architecture) |
| 5ï¸âƒ£ | Test your knowledge | [Interview Q&A](./Vision_Transformers/interview_qa/README.md) |

---

<div align="center">

## ğŸ“„ Source Papers

</div>

| Survey | Paper | Authors | Year |
|:-------|:------|:--------|:----:|
| Efficient Attention | [Attention Survey](https://attention-survey.github.io) | Zhang et al. | 2025 |
| Faster & Lighter | [ACM Computing Surveys](./Faster%20and%20Lighter%20Transformers/A%20Practical%20Survey%20on%20Faster%20and%20Lighter%20Transformers.pdf) | Fournier et al. | 2023 |

---


